\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{cite}
\usepackage{balance}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{booktabs} % 用于专业排版的表格宏包
\usepackage{multirow} % 用于合并单元格的宏包
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Video Retrieval of Autonomous Driving Scenarios Based on Large Models}
\author{12310520 Rui Yuhan   12310437 Qiao Shihan}


\maketitle

\begin{abstract}
Video retrieval in autonomous driving scenarios demands models to comprehend complex visual environments and align them with detailed textual descriptions. While multimodal large language models (MLLMs) have demonstrated potential in vision-language tasks, their application to domain-specific video retrieval remains underexplored. Furthermore, the scarcity of publicly available high-quality annotated video data in the autonomous driving domain makes it challenging for models to accurately learn video content semantics from limited data. Additionally, significant differences exist between autonomous driving videos and those in general video datasets, leading to suboptimal performance of generic large models on autonomous driving datasets.

To address these challenges, we evaluate the performance of two distinct pretrained MLLMs on a carefully curated autonomous driving dataset. This dataset features high-quality captions that have undergone multi-round optimization, combining human annotation expertise with advanced automated labeling techniques, while also serving as a benchmark platform for exploring more efficient annotation strategies. Experimental results demonstrate that our approach effectively retrieves relevant video segments based on textual queries, highlighting the potential of MLLMs to enhance retrieval accuracy and scalability.

\end{abstract}

\begin{IEEEkeywords}
Autonomous Driving Video Retrieval, Multimodal Large Language Models (MLLMs), Video Understanding, Automated Annotation Optimization
\end{IEEEkeywords}


\section{Introduction}

\section{Two knowledge distillation methods used}
\subsection{Research Background and Motivation of Knowledge Distillation}
With the rapid advancement of large-scale foundation models, Vision-Language Models (VLMs), and Large Language Models (LLMs), deploying these models on edge devices faces
significant challenges due to high computational and memory requirements. Knowledge Distillation (KD), first proposed by Hinton et al.~\cite{hinton2015distilling}, addresses this issue by training a lightweight
student network under the supervision of a complex teacher model. Unlike compression methods that modify network structures, KD transfers knowledge by having the student mimic the teacher’s output distribution as soft labels, enabling effective model compression while maintaining performance.
Additionally, KD facilitates knowledge transfer from source tasks to target tasks with limited labeled data, making it particularly valuable for domain-specific applications such as autonomous driving video retrieval.

\subsubsection{Mainstream Knowledge Distillation Approaches}
Knowledge distillation methods can be categorized based on the source of knowledge being distilled~\cite{mansourian2025comprehensive}. 
The three primary categories are: 
\textbf{logit-based distillation}, which transfers final predictions using softmax with temperature to create soft labels; 
\textbf{feature-based distillation}, which transfers intermediate layer representations and attention maps, providing richer information than logits alone; 
and \textbf{similarity-based distillation}, which transfers structural knowledge through pairwise similarities between features, channels, or instances. 
Additionally, distillation can be organized by training schemes: 
\textbf{offline distillation} (pre-trained teacher with frozen weights), \textbf{online distillation} (simultaneous training), and \textbf{self-distillation} (same network transferring knowledge across layers or stages). 
Other notable approaches include \textbf{attention-based distillation}for transferring focus patterns, \textbf{contrastive distillation} leveraging contrastive learning principles, and \textbf{cross-modal distillation} for transferring knowledge between different modalities, which is particularly relevant for multimodal tasks like video retrieval.

\subsection{DLDKD}

DLDKD (Dual Learning with Dynamic Knowledge Distillation)~\cite{dong2023dual} employs a dual-stream teacher-student framework that transfers knowledge from a large teacher model to a lightweight student model through multi-level distillation.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{images/dldkd_framework.png}
\caption{The framework of DLDKD.}
\label{fig:dldkd}
\end{figure}

DLDKD performs distillation at multiple levels: feature-level distillation transfers intermediate representations, logit-level distillation aligns output distributions, and attention-level distillation preserves attention patterns.

\subsubsection{Loss Functions}
The DLDKD framework employs a dual learning objective that combines knowledge from the teacher model with task-specific learning through two student branches. The total loss function is defined as:

\begin{equation}
\mathcal{L}_{total} = \lambda_I \mathcal{L}_I + \lambda_E \mathcal{L}_E + \mathcal{L}_{task}
\end{equation}

where $\lambda_I$ and $\lambda_E$ are dynamic weights that adjust the contribution of the inheritance branch and exploration branch losses, respectively.

\textbf{Inheritance Student Branch Loss ($\mathcal{L}_I$):} The inheritance branch learns to absorb knowledge from the teacher model through multi-level distillation. The loss combines feature-level, logit-level, and attention-level distillation:

\begin{equation}
\mathcal{L}_I = \alpha_f \mathcal{L}_{feat} + \alpha_l \mathcal{L}_{logit} + \alpha_a \mathcal{L}_{attn}
\end{equation}

where $\alpha_f$, $\alpha_l$, and $\alpha_a$ are weights for feature-level, logit-level, and attention-level distillation losses, respectively.

The feature-level distillation loss aligns intermediate representations between teacher and student:
\begin{equation}
\mathcal{L}_{feat} = \frac{1}{N}\sum_{i=1}^{N} \|F_t^{(i)} - F_s^{(i)}\|_2^2
\end{equation}
where $F_t^{(i)}$ and $F_s^{(i)}$ denote the feature representations at layer $i$ from the teacher and student models, respectively, and $N$ is the number of layers used for feature distillation.

The logit-level distillation loss uses KL divergence to align output distributions:
\begin{equation}
\mathcal{L}_{logit} = \text{KL}(\text{softmax}(z_t/\tau) \| \text{softmax}(z_s/\tau))
\end{equation}
where $z_t$ and $z_s$ are the logits from teacher and student models, and $\tau$ is the temperature parameter for softening the distributions.

The attention-level distillation loss preserves the teacher's attention patterns:
\begin{equation}
\mathcal{L}_{attn} = \frac{1}{L}\sum_{l=1}^{L} \|A_t^{(l)} - A_s^{(l)}\|_F^2
\end{equation}
where $A_t^{(l)}$ and $A_s^{(l)}$ are attention maps at layer $l$ from teacher and student models, $L$ is the number of attention layers, and $\|\cdot\|_F$ denotes the Frobenius norm.

\textbf{Exploration Student Branch Loss ($\mathcal{L}_E$):} The exploration branch focuses on task-specific learning to capture domain-specific patterns that may not be fully represented in the teacher model:
\begin{equation}
\mathcal{L}_E = -\frac{1}{B}\sum_{i=1}^{B} \log \frac{\exp(\text{sim}(v_i, t_i^+)/\tau_E)}{\sum_{j=1}^{B} \exp(\text{sim}(v_i, t_j)/\tau_E)}
\end{equation}
where $B$ is the batch size, $v_i$ is the video embedding, $t_i^+$ is the positive text query, $\text{sim}(\cdot,\cdot)$ computes cosine similarity, and $\tau_E$ is the temperature parameter for the exploration branch.

\textbf{Task-Specific Loss ($\mathcal{L}_{task}$):} The task loss ensures that the final model performs well on the video-text retrieval task:
\begin{equation}
\mathcal{L}_{task} = \mathcal{L}_{retrieval} = -\log P(v^+ | t) - \log P(t^+ | v)
\end{equation}
where $P(v^+ | t)$ and $P(t^+ | v)$ represent the probabilities of retrieving the correct video given text query and vice versa.

\textbf{Dynamic Weight Adjustment:} The weights $\lambda_I$ and $\lambda_E$ are dynamically adjusted during training based on the performance of each branch. This allows the framework to automatically balance between teacher knowledge inheritance and task-specific exploration:
\begin{equation}
\lambda_I^{(t)} = \lambda_I^{(0)} \cdot \exp(-\gamma_I \cdot t), \quad \lambda_E^{(t)} = \lambda_E^{(0)} \cdot (1 + \gamma_E \cdot t)
\end{equation}
where $t$ is the training epoch, $\lambda_I^{(0)}$ and $\lambda_E^{(0)}$ are initial weights, and $\gamma_I$, $\gamma_E$ are decay/growth rates that control the dynamic adjustment schedule.

\subsubsection{Feature Pre-Extraction Strategy}
DLDKD employs feature pre-extraction to accelerate training: teacher model features are pre-computed and cached for all training samples before distillation training, eliminating repeated forward passes through the teacher model during training. In the subsequent modifications to TeachCLIP, we will incorporate this feature pre-extraction strategy to shorten the training time.

\subsection{TeachCLIP}
\subsubsection{Inspiration}
Existing VTR models generally fall into two categories: lightweight global feature models (e.g., CLIP4Clip) and heavy fine-grained models (e.g., X-CLIP). The former is computationally efficient but often lacks frame-level details, resulting in suboptimal retrieval accuracy. The latter leverages frame-level interactions to enhance performance but incurs high computational costs.

\begin{figure}[h]
\centering
\subfloat[CLIP4Clip Framework]{\includegraphics[width=0.8\linewidth]{images/clip4clip_framework.png}}
\\
\subfloat[X-CLIP Framework]{\includegraphics[width=0.8\linewidth]{images/xclip_framework.png}}
\caption{Comparison of CLIP4Clip and X-CLIP frameworks. Both use similar bottom-up encoding but differ in upper-layer feature usage.}
\label{fig:inspiration}
\end{figure}

In terms of vision and text encoding, both CLIP4Clip and X-CLIP share nearly identical bottom-level encoding mechanisms (Frame $\to$ Patch $\to$ ViT, Word $\to$ Embedding $\to$ Transformer). The primary distinction lies in how they utilize these features in the upper layers: CLIP4Clip performs pooling to retain only global video vectors, whereas X-CLIP preserves fine-grained features and performs multi-granularity alignment.

TeachCLIP aims to bridge this gap by distilling the fine-grained alignment capability of heavy models into a lightweight student model.

\subsubsection{Framework}
The TeachCLIP framework is designed to transfer the fine-grained knowledge from a teacher model to a student model while maintaining high inference efficiency.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{images/teachclip_framework.png}
\caption{The framework of TeachCLIP.}
\label{fig:teachclip}
\end{figure}

\textbf{Student Model:} The student model is built upon CLIP4Clip. It samples the video into frames, processes each frame through the CLIP ViT to obtain frame features, and enhances them via a temporal Transformer. A key innovation is replacing the original mean pooling with Attentional Frame-Feature Aggregation (AFA). AFA generates frame weights $\{w_i\}$ to compute a weighted sum of frame features:
\begin{equation}
\phi(x) = \sum_{i=1}^m w_i \cdot \phi_i
\end{equation}
The AFA module consists of a lightweight structure (Linear $\to$ ReLU $\to$ Linear $\to$ Softmax) and introduces negligible parameters.

\textbf{Teacher Model:} The teacher model provides two types of supervision:
\begin{itemize}
    \item \textbf{Video-level soft labels ($y_c$):} The teacher provides the distribution of video-text correlations to guide the student's ranking.
    \item \textbf{Frame-level soft labels ($y_f$):} The teacher provides the distribution of frame-text correlations to guide the student's frame weight allocation in AFA.
\end{itemize}

\textbf{Learning Objectives:} The training involves three losses:

1) \textit{Frame-level Distillation ($\ell_{FgT}$):} Optimizes the AFA weights to match the teacher's frame-text similarity distribution. The loss function is defined as:
\begin{equation}
\ell_{FgT} = -\frac{1}{b}\sum_{i=1}^{b}\sum_{k=1}^{m} y_f(f_{i,k},t_i)\,\log w_{i,k}
\end{equation}
where $b$ is the batch size, $m$ is the number of frames, and $y_f$ represents the frame-text similarity from the teacher.

2) \textit{Video-level Distillation ($\ell_{CgT}$):} Aligns the student's video-text similarity matrix with the teacher's multi-grained similarity matrix using the Pearson distance $d_p$:
\begin{equation}
\begin{aligned}
\ell_{CgT} = \frac{1}{b}\sum_i d_p(\sigma(B_{i,\cdot}),\sigma(y_c(v_i,\cdot))) \\
+ \frac{1}{b}\sum_j d_p(\sigma(B_{\cdot,j}),\sigma(y_c(\cdot,t_j)))
\end{aligned}
\end{equation}
where $\sigma$ denotes the softmax function and $B$ is the similarity matrix of the student.

3) \textit{Contrastive Learning ($\ell_{IN}$):} Standard maximization of similarity for positive pairs and minimization for negative pairs, using symmetric InfoNCE loss:
\begin{equation}
\ell_{IN} = \frac{1}{2}\Big[ \ell_{NCE}^{v \to t} + \ell_{NCE}^{t \to v} \Big]
\end{equation}
where
\begin{equation}
\ell_{NCE}^{v \to t} = -\frac{1}{b}\sum_{i=1}^{b} \log \frac{\exp(B_{ii}/\tau)}{\sum_{j=1}^{b} \exp(B_{ij}/\tau)}
\end{equation}
and $\ell_{NCE}^{t \to v}$ is defined symmetrically.

The total loss is $\ell = \ell_{CgT} + \ell_{FgT} + \ell_{IN}$. During inference, the teacher model is discarded, and only the lightweight student model with AFA is used.

\subsubsection{TeachCLIP vs X-CLIP Parameters and FLOPs Comparison}
Table \ref{tab:teachclip_vs_xclip} presents the comparison of parameters and FLOPs between TeachCLIP and the teacher model (X-CLIP).

\begin{table}[h]
\centering
\caption{Comparison of Parameters and FLOPs}
\label{tab:teachclip_vs_xclip}
\begin{tabular}{p{1.4cm} p{1.2cm} p{2.2cm} p{3.2cm}}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{FLOPs (Inference)} & \textbf{Description} \\
\midrule
\textbf{TeachCLIP} & $\approx$ 200M & 53.65G (12 frames) & Student model, based on CLIP4Clip + AFA, maintains lightweight inference \\
\midrule
\textbf{X-CLIP (Teacher)} & $\approx$ 220M & 145G (8 frames) / 287G (16 frames) & Teacher model, introduces multi-grained contrastive similarity, higher inference cost \\
\bottomrule
\end{tabular}
\end{table}

\section{Alterations on TeachCLIP}
\subsection{On student model}
\subsection{On teacher model}

\section{Post-Training Quantization of VAST Model using MS-Swift}

In this section, we apply post-training quantization methods introduced in MS-Swift~\cite{zhao2024swiftascalablelightweightinfrastructure} to compress the VAST model~\cite{chen2023vast} for video-text retrieval tasks. Post-training quantization is a model compression technique that reduces the precision of model weights and activations after the model has been fully trained, enabling efficient deployment on resource-constrained devices without requiring retraining from scratch.

\subsection{MS-Swift Quantization Architecture and Methodology}

MS-Swift (ModelScope Swift)~\cite{zhao2024swiftascalablelightweightinfrastructure} provides a comprehensive quantization framework that supports various quantization strategies for large-scale models. The quantization architecture in MS-Swift is designed to be modular and flexible, allowing users to apply different quantization schemes to different parts of the model based on their sensitivity to precision loss.

The key components of MS-Swift's quantization framework include:

\textbf{Quantization Strategies:} MS-Swift supports multiple quantization approaches, including uniform quantization, dynamic quantization, and static quantization. For post-training quantization, the framework primarily employs static quantization, where quantization parameters (scales and zero-points) are calibrated using a small representative dataset. This approach balances model compression ratio and accuracy retention.

\textbf{Precision Schemes:} The framework supports various bit-width configurations, including 8-bit integer (INT8) quantization for weights and activations, mixed-precision quantization where critical layers maintain higher precision (FP16 or FP32), and adaptive quantization that automatically selects optimal bit-widths for different layers based on their contribution to the final output.

\textbf{Calibration Process:} MS-Swift's quantization pipeline includes a calibration phase where a small subset of training data is used to compute quantization parameters. During calibration, the framework collects activation statistics to determine appropriate scale factors and zero-points that minimize quantization error. The calibration process ensures that the quantized model maintains performance close to the full-precision model.

\textbf{Quantization-Aware Operations:} The framework provides optimized implementations of common operations (matrix multiplication, convolution, layer normalization) that work efficiently with quantized tensors, ensuring that the quantized model can leverage hardware acceleration available on modern processors.

\subsection{VAST Architecture for Video-Text Retrieval}

VAST (Vision-Audio-Subtitle-Text)~\cite{chen2023vast} is an omni-modality foundation model. The VAST framework architecture is illustrated in Figure \ref{fig:vast}.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{images/vast_framework.png}
\caption{The framework of VAST.}
\label{fig:vast}
\end{figure}

For video-text retrieval, VAST employs dual encoders: a video encoder that processes frames through a ViT architecture with temporal modeling, and a text encoder based on transformer architecture. The model learns cross-modal alignment in a shared embedding space, using contrastive learning to maximize similarity between matching video-text pairs.

The VAST model achieves strong performance in video-text retrieval after fine-tuning on domain-specific data. Given the model's size, there is room for further lightweight optimization to better adapt to real-time applications. We therefore apply post-training quantization to obtain smaller model weights while maintaining competitive performance.

\subsection{Post-Training Quantization Experimental Process}

We perform post-training quantization on the fine-tuned VAST model weights using MS-Swift's quantization framework. The experimental process consists of the following steps:

\textbf{Model Preparation:} We start with a VAST model that has been fine-tuned on our target video-text retrieval dataset. The fine-tuned model serves as the full-precision baseline, providing reference accuracy metrics for comparison with the quantized version.

\textbf{Calibration Dataset Preparation:} A representative subset of the training dataset is selected for calibration. This calibration set typically consists of 100-1000 samples that cover the diversity of the full dataset. The calibration data is used to compute quantization parameters without requiring gradient computation or backpropagation.

\textbf{Quantization Configuration:} We configure the quantization settings using MS-Swift's API, specifying:
\begin{itemize}
    \item Target precision: INT8 for both weights and activations
    \item Quantization scheme: Static quantization with calibration
    \item Layer-wise precision: Maintaining FP32 precision for sensitive layers (e.g., layer normalization, final projection layers)
    \item Calibration method: Min-max calibration or percentile-based calibration
\end{itemize}

\textbf{Quantization Execution:} The quantization process is performed in two phases:
\begin{enumerate}
    \item \textit{Calibration Phase:} The calibration dataset is passed through the model in inference mode. Activation statistics are collected for each layer to determine optimal quantization scales and zero-points.
    \item \textit{Conversion Phase:} The full-precision model weights are quantized to INT8 using the calibrated parameters, and the model graph is transformed to use quantized operations.
\end{enumerate}

\textbf{Evaluation:} The quantized model is evaluated on the test set to assess the impact of quantization on retrieval performance. Key metrics include retrieval accuracy (Recall@K for K=1, 5, 10), mean reciprocal rank (MRR), and inference latency. Additionally, we measure the model size reduction and memory footprint to quantify the compression benefits.

\textbf{Performance Optimization:} If the quantized model shows significant accuracy degradation, we apply fine-tuning strategies such as quantization-aware fine-tuning (QAT) on the quantized model, or adjust the quantization configuration to use mixed-precision quantization for critical layers.

The quantized VAST model, achieved through MS-Swift's post-training quantization, enables efficient deployment while maintaining competitive retrieval performance, making it suitable for real-time video-text retrieval applications in autonomous driving scenarios.


\section{Conclusion}

Autonomous driving video retrieval presents unique challenges due to the complexity of real-world driving scenarios and the scarcity of high-quality annotated datasets. In this work, we explored the potential of multimodal large language models (MLLMs) to bridge the gap between visual perception and textual understanding in autonomous driving contexts. By leveraging a carefully curated dataset with optimized annotations, combining human expertise and automated refinement using Video-LLama2, we demonstrated that pretrained MLLMs can be effectively adapted for domain-specific video retrieval tasks.

Our experiments with two state-of-the-art MLLMs (Vast and Clip-Vip) revealed that fine-tuning on high-quality annotated data significantly improves retrieval accuracy, validating the importance of robust dataset construction. Additionally, our work establishes a benchmark for future research in autonomous driving video understanding, providing a foundation for developing more interpretable and scalable retrieval systems.

Moving forward, we anticipate that further advancements in MLLMs, combined with more sophisticated annotation pipelines, will enhance the generalization capabilities of autonomous driving systems. Future research could explore dynamic annotation strategies, real-time retrieval optimization, and the integration of multi-sensor data (e.g., LiDAR and radar) to further improve model performance in complex driving environments.

Ultimately, this study highlights the critical role of high-quality data and domain-specific adaptation in advancing autonomous driving technologies, paving the way for safer and more intelligent transportation systems.






\balance
\bibliographystyle{IEEEtran}
\bibliography{reference}

\end{document}